---
title: "Parallel Computation"
output:
  html_document: default
  pdf_document: default
date: "02-04-2020"
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(cache = TRUE)
```

```{r, message = TRUE}
library(tidyverse)
```

# Parallel Computation


## Implict Parallel

- BLAS (Basic Linear Algebra Subroutines)
  - CRAN R shippings with a [version](https://github.com/wch/r-source/tree/trunk/src/extra/blas) of single threaded BLAS library.
  - [Microsoft R Open](https://mran.microsoft.com/open) ships with Intel MKL (Win/Linux) / Accelerate ML (macOS) BLAS libraries.
  - on macOS, R could be configured to use the optimized BLAS from Appleâ€™s Accelerate framework
  - We could only install R with different BLAS libraries such as [openblas](https://github.com/xianyi/OpenBLAS) or [ATLAS](http://math-atlas.sourceforge.net/)


## Embarrassingly Parallel

Also called perfectly parallel, delightfully parallel or pleasingly parallel. 

> An embarrassingly parallel task can be considered a trivial case - little or no manipulation is needed to separate the problem into a number of parallel tasks.

A bit deroute first - revisit some of our old friends `map` and `map_*` in `purrr`.

```{r}
1:4 %>% map(function(x) x^2)
1:4 %>% map_dbl(function(x) x^2)
1:4 %>% map_dbl(~ .^2)
```

These are the base R equivalence.
```{r}
1:4 %>% lapply(function(x) x^2)
1:4 %>% sapply(function(x) x^2)
```


Suppose we have a list of vectors and we want to operation some operation on each vector.

```{r}
# it is a slow operation, imagine that in real applications, it could take a few minutes
slow_task <- function(x) {
  sum(x %o% x)
}

list_of_vectors <- replicate(10, list(rnorm(5000)))
list_of_vectors %>% glimpse()
```

```{r, eval = FALSE}
list_of_vectors %>% map_dbl(slow_task)
```

However, these commands only run in a single process, it means, if the list is doubled, the time is also at least doubled.

```{r}
system.time({
  list_of_vectors %>% map_dbl(slow_task)
})

# double the list
longer_list_of_vectors <- c(list_of_vectors, list_of_vectors)
system.time({
  longer_list_of_vectors %>% map_dbl(slow_task)
})
```

We are hoping to use multiple processes to speed up the job. The traditional way is to use the `parallel` package.

## The package `parallel`

```{r}
library(parallel)
```

Consider again the above list_vector example,
```{r}
# the number of cores we have
detectCores()
# it will create a socket cluster on my own computer
cl <- makeCluster(4)
parLapply(cl, list_of_vectors, slow_task)
# or if you want simplified result
parSapply(cl, list_of_vectors, slow_task)
# stop the cluster after use
stopCluster(cl)
```

Remark: you don't have to make and stop clusters for every operation, you could make a cluster in the very beginning of your script and close it at the very end.


Let's test the speed improvement

```{r}
run_each <- function(x, fun, n_cores) {
  cl <- makeCluster(n_cores)
  result <- parLapply(cl, x, fun)
  stopCluster(cl)
  result
}

system.time(run_each(longer_list_of_vectors, slow_task, 2))
system.time(run_each(longer_list_of_vectors, slow_task, 8))
```


## Processing Chunk

The iteratable is divided into chunks before sending the chunks to the workers. `Sys.getpid()` tells us the process id of a worker.

```{r}
cl <- makeCluster(4)
```

```{r}
parSapply(cl, 1:10, function(x) {
  Sys.getpid()
})
parSapply(cl, 1:10, function(x) {
  Sys.getpid()
},
chunk.size = 2
)
parSapply(cl, 1:10, function(x) {
  Sys.getpid()
},
chunk.size = 1
)
```

```{r}
stopCluster(cl)
```


## Load balancing


`parLapply` pre-schedules the tasks to each work. It could be suboptimal when different tasks require different amount of time to complete.

```{r}
cl <- makeCluster(4)
```

```{r}
x <- c(3, 3, 1, 1, 1, 1, 1, 1)
pause <- function(x) {
  Sys.sleep(x)
}

system.time({
  map(x, pause)
})

system.time({
  parLapply(cl, x, pause)
})
system.time({
  parLapply(cl, x, pause, chunk.size = 2)
})
system.time({
  parLapply(cl, x, pause, chunk.size = 1)
})
```

Instead of preshceduling the tasks, a task could also be assigned to a free worker dynamically using `parLapplyLB`.

```{r}
system.time({
  parLapplyLB(cl, x, pause)
})
```
Note that it only takes 3 seconds now.


```{r}
stopCluster(cl)
```


## Interact directly with the workers

We just saw an quick example on using `parLapply`. Let's try a few more things.

```{r}
cl <- makeCluster(4)
```

We could run some arbitrary commands on each of the workers
```{r}
clusterEvalQ(cl, {
  x <- rnorm(100)
  mean(x)
})
```

```{r}
clusterEvalQ(cl, {
  Sys.getpid()
})
```

Global variables in master are not exported to the worker automatically (the same is true for `parLapply`)
```{r, error = TRUE}
y <- 3
clusterEvalQ(cl, {
  y + 1
})
```
`clusterExport` exports the global variables to each worker.
```{r}
clusterExport(cl, "y")
clusterEvalQ(cl, {
  y + 1
})
```


If you want to set a random seed, the following doesn't work because each work returns the same result.


```{r}
# wrong
set.seed(123)
clusterEvalQ(cl, {
  rnorm(5)
})
# wrong again
clusterEvalQ(cl, {
  set.seed(123)
  rnorm(5)
})
```

```{r}
clusterSetRNGStream(cl, 123)
clusterEvalQ(cl, {
  rnorm(5)
})
```

```{r}
# do not forget to close the cluster
stopCluster(cl)
```


## Another pouplar package: `foreach`

As we have seen a bit earlier, we might need to use `clusterExport` to export certain gloabl variables. It would be a bit cumbersome. To reduce the extra steps, we could consider `foreach` and `doParallel`. They will send all the globals to the workers before running the tasks.

```{r}
library(foreach)
library(doParallel)
```

```{r}
cl <- makeCluster(4)
registerDoParallel(cl)
m <- matrix(rnorm(9), 3, 3)
# matrix m is sent to the workers implictly
# in fact, all globals are sent to the workers in default
foreach(i = seq_len(nrow(m)), .combine = c) %dopar% {
  sum(m[i, ])
}
stopCluster(cl)
```

However, it is not quite "functional" as `map`, `lapply` and `parLapply`.


## `map` or `lapply` like syntax

### `mclapply` from `parallel` (unix / macOS only)

Remark: `mclapply` relies on forking, it means that it doesn't work on Windows. We will discuss a cross platform approach.)

```{r}
# in default, `mclapply` uses 2 cores
system.time({
  mclapply(
    c(2, 2, 2, 2),
    function(x) Sys.sleep(x)
  )
})

system.time({
  mclapply(
    c(3, 3, 1, 1, 1, 1, 1, 1),
    function(x) Sys.sleep(x),
    mc.preschedule = FALSE, #  set FALSE to enable load balancing
    mc.cores = 4
  )
})
```

### package `furrr`

`furrr` provides functions which are very similar to those in `purrr`.

```{r}
library(furrr)
```

```{r, warning = FALSE}
# to use 4 workers, `plan(multiprocess)` will use all the avaliable workers
plan(multiprocess, workers = 4)
system.time({
  future_map(
    c(2, 2, 2, 2),
    ~ Sys.sleep(.)
  )
})
```

`future_map` has a family of type speific functions. For example,
```{r}
future_map_dbl(list(1:10, 11:20, 21:30, 31:41), ~ sum(.))
```


load balanacing in `future_map`

```{r}
# without load balanacing
system.time({
  future_map(
    c(3, 3, 1, 1, 1, 1, 1, 1),
    ~ Sys.sleep(.)
  )
})

# with load balanacing
system.time({
  future_map(
    c(3, 3, 1, 1, 1, 1, 1, 1),
    ~ Sys.sleep(.),
    .options = future_options(scheduling = FALSE)
  )
})
```


## The honorable mention `multidplyr`


The package `multidplyr` is tidyverse solution to multiprocess data management. However, it is not released to CRAN yet, it means we need to install it from its github repo by using the package `remotes`.

```{r, eval = FALSE}
remotes::install_github("tidyverse/multidplyr")
```

Since it is still under developement, use it with caution!


```{r}
library(multidplyr)
```

There are two ways to get data to the workers in cluster:

- `partition()` a data frame that already loaded in the interactive process.
- Load a different subset of the data in each worker.


Use `partition()` to send data to workers
```{r}
library(nycflights13)

cluster <- new_cluster(4)

flights %>%
  group_by(dest, origin) %>%
  partition(cluster) %>%
  summarize(air_time = mean(air_time, na.rm = TRUE))
```


Load data in each worker
```{r}
cluster <- new_cluster(4)

cluster %>% cluster_library(c("tidyverse", "nycflights13"))

cluster %>%
  cluster_assign_partition(destination = unique(flights$dest))

# let's check the `destimation` variable in the workers
cluster %>%
  cluster_call(destination)

cluster %>% cluster_send({
  df <- flights %>% filter(dest %in% destination)
})

cluster %>%
  party_df("df") %>%
  group_by(dest, origin) %>%
  summarize(air_time = mean(air_time, na.rm = TRUE)) %>%
  collect()
```




Reference:

- R Programming for Data Science https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html
